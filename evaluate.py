"""
Model Evaluation Module
ëª¨ë¸ í‰ê°€ ì›Œí¬í”Œë¡œìš° ë° ì‹œê°í™”
UK GeoAccent í”„ë¡œì íŠ¸ìš© ì™„ì „í•œ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
"""


import os
import sys
import argparse
import json
from pathlib import Path
from datetime import datetime

import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from typing import List, Dict, Optional, Tuple, Union

# metrics ëª¨ë“ˆì—ì„œ í•¨ìˆ˜ import
from metrics import (
    calculate_accuracy,
    calculate_per_class_accuracy,
    calculate_f1_macro,
    calculate_f1_weighted,
    calculate_per_class_f1,
    calculate_precision_recall,
    get_confusion_matrix,
    get_classification_report,
    calculate_all_metrics
)


class ModelEvaluator:
    """ëª¨ë¸ í‰ê°€ë¥¼ ìœ„í•œ ì¢…í•© í´ë˜ìŠ¤"""
    
    def __init__(self, 
                 y_true: Optional[np.ndarray] = None, 
                 y_pred: Optional[np.ndarray] = None, 
                 class_names: Optional[List[str]] = None,
                 model: Optional[nn.Module] = None,
                 test_loader: Optional[DataLoader] = None,
                 device: str = 'cuda'):
        """
        Args:
            y_true: ì‹¤ì œ ë ˆì´ë¸” (ì˜µì…˜)
            y_pred: ì˜ˆì¸¡ ë ˆì´ë¸” (ì˜µì…˜)
            class_names: í´ë˜ìŠ¤ ì´ë¦„ ë¦¬ìŠ¤íŠ¸
            model: PyTorch ëª¨ë¸ (ì˜µì…˜)
            test_loader: í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë” (ì˜µì…˜)
            device: 'cuda' ë˜ëŠ” 'cpu'
        """
        self.device = device if torch.cuda.is_available() else 'cpu'
        self.model = model
        self.test_loader = test_loader
        
        # ëª¨ë¸ì´ ì œê³µëœ ê²½ìš° í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
        if self.model is not None:
            self.model.eval()
            self.model.to(self.device)
        
        # ë ˆì´ë¸” ì„¤ì •
        if y_true is not None and y_pred is not None:
            self.y_true = np.array(y_true)
            self.y_pred = np.array(y_pred)
        else:
            self.y_true = None
            self.y_pred = None
        
        # í´ë˜ìŠ¤ ì´ë¦„ ì„¤ì •
        if self.y_true is not None:
            self.classes = np.unique(np.concatenate([self.y_true, self.y_pred]))
            if class_names is None:
                self.class_names = [f"Class {i}" for i in self.classes]
            else:
                self.class_names = class_names
        else:
            self.class_names = class_names if class_names else []
        
        # ë©”íŠ¸ë¦­ ì €ì¥
        self.metrics = None
        self.y_proba = None
    
    def predict_from_loader(self, verbose: bool = True) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """
        ë°ì´í„° ë¡œë”ë¡œë¶€í„° ì˜ˆì¸¡ ìˆ˜í–‰
        
        Args:
            verbose: ì§„í–‰ ìƒí™© í‘œì‹œ ì—¬ë¶€
        
        Returns:
            (y_true, y_pred, y_proba) tuple
        """
        if self.model is None or self.test_loader is None:
            raise ValueError("Model and test_loader must be provided for prediction")
        
        if verbose:
            print("\n" + "="*70)
            print("ğŸ”® Running Predictions...")
            print("="*70)
        
        all_labels = []
        all_preds = []
        all_probas = []
        
        with torch.no_grad():
            iterator = tqdm(self.test_loader, desc="Predicting") if verbose else self.test_loader
            
            for batch in iterator:
                # ì…ë ¥ ë°ì´í„° ì¶”ì¶œ
                input_values = batch['input_values'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
            
                # ë ˆì´ë¸” ì„ íƒ
                if task == 'region':
                    labels = batch['region_labels'].to(self.device)
                else:  # gender
                    labels = batch['gender_labels'].to(self.device)
                
                # ì˜ˆì¸¡
                outputs = self.model(input_values)
                
                # ì¶œë ¥ ì²˜ë¦¬
                if outputs.dim() == 1 or outputs.shape[1] == 1:
                    # Binary classification
                    probas = torch.sigmoid(outputs.squeeze())
                    preds = (probas > 0.5).long()
                    probas = torch.stack([1-probas, probas], dim=1)
                else:
                    # Multi-class classification
                    probas = torch.softmax(outputs, dim=1)
                    preds = torch.argmax(outputs, dim=1)
            
            # ê²°ê³¼ ì €ì¥
            all_labels.append(labels.cpu().numpy())
            all_preds.append(preds.cpu().numpy())
            all_probas.append(probas.cpu().numpy())
        
        # ê²°í•©
        self.y_true = np.concatenate(all_labels)
        self.y_pred = np.concatenate(all_preds)
        self.y_proba = np.concatenate(all_probas)
        
        if verbose:
            print(f"âœ… Predictions completed: {len(self.y_true)} samples")
        
        return self.y_true, self.y_pred, self.y_proba
    
    def calculate_metrics(self) -> Dict:
        """ëª¨ë“  ë©”íŠ¸ë¦­ ê³„ì‚°"""
        if self.y_true is None or self.y_pred is None:
            if self.model is not None and self.test_loader is not None:
                self.predict_from_loader()
            else:
                raise ValueError("No predictions available. Provide y_true/y_pred or model/test_loader")
        
        print("\n" + "="*70)
        print("ğŸ“Š Calculating Metrics...")
        print("="*70)
        
        self.metrics = calculate_all_metrics(
            self.y_true, 
            self.y_pred, 
            self.class_names
        )
        
        return self.metrics
    
    def print_summary(self):
        """ì „ì²´ ë©”íŠ¸ë¦­ ìš”ì•½ ì¶œë ¥"""
        if self.metrics is None:
            self.calculate_metrics()
        
        print("\n" + "="*70)
        print("ğŸ“ˆ MODEL EVALUATION SUMMARY")
        print("="*70)
        
        # Overall Accuracy
        print(f"\nğŸ“Š Overall Accuracy: {self.metrics['overall_accuracy']:.4f} ({self.metrics['overall_accuracy']*100:.2f}%)")
        
        # Per-Class Accuracy
        print("\nğŸ“‹ Per-Class Accuracy:")
        for class_name, acc in self.metrics['per_class_accuracy'].items():
            support = np.sum(self.y_true == self.class_names.index(class_name))
            print(f"  â€¢ {class_name:20s}: {acc:.4f} ({acc*100:.2f}%) | Support: {support}")
        
        # F1 Scores
        print(f"\nğŸ“ˆ F1 Score (Macro):    {self.metrics['f1_macro']:.4f}")
        print(f"ğŸ“ˆ F1 Score (Weighted): {self.metrics['f1_weighted']:.4f}")
        
        # Per-Class F1
        print("\nğŸ“‹ Per-Class F1 Score:")
        for class_name, f1 in self.metrics['per_class_f1'].items():
            print(f"  â€¢ {class_name:20s}: {f1:.4f}")
        
        # Precision & Recall
        pr = self.metrics['precision_recall']
        print(f"\nğŸ¯ Precision (Macro):   {pr['precision']:.4f}")
        print(f"ğŸ¯ Recall (Macro):      {pr['recall']:.4f}")
    
    def print_classification_report(self):
        """Classification report ì¶œë ¥"""
        print("\n" + "="*70)
        print("ğŸ“‹ DETAILED CLASSIFICATION REPORT")
        print("="*70)
        report = get_classification_report(
            self.y_true, 
            self.y_pred, 
            self.class_names, 
            output_dict=False
        )
        print(report)
    
    def plot_confusion_matrix(self, 
                             figsize: Tuple[int, int] = (10, 8),
                             cmap: str = 'Blues',
                             save_path: Optional[str] = None,
                             show_percentages: bool = False,
                             normalize: bool = False) -> plt.Figure:
        """
        Confusion matrix ì‹œê°í™”
        
        Args:
            figsize: Figure í¬ê¸°
            cmap: Color map
            save_path: ì €ì¥ ê²½ë¡œ
            show_percentages: ë°±ë¶„ìœ¨ í‘œì‹œ ì—¬ë¶€
            normalize: ì •ê·œí™” ì—¬ë¶€
        
        Returns:
            Figure object
        """
        if self.metrics is None:
            self.calculate_metrics()
        
        cm = self.metrics['confusion_matrix']
        
        fig, ax = plt.subplots(figsize=figsize)
        
        if normalize:
            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
            fmt = '.2f'
        elif show_percentages:
            cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100
            annot = np.array([[f'{int(count)}\n({percent:.1f}%)' 
                              for count, percent in zip(row_counts, row_percents)]
                             for row_counts, row_percents in zip(cm, cm_percent)])
            sns.heatmap(cm, annot=annot, fmt='', cmap=cmap,
                       xticklabels=self.class_names,
                       yticklabels=self.class_names,
                       cbar_kws={'label': 'Count'},
                       ax=ax)
            ax.set_xlabel('Predicted Label', fontsize=12, fontweight='bold')
            ax.set_ylabel('True Label', fontsize=12, fontweight='bold')
            ax.set_title('Confusion Matrix', fontsize=14, fontweight='bold', pad=20)
            plt.tight_layout()
            
            if save_path:
                plt.savefig(save_path, dpi=300, bbox_inches='tight')
                print(f"âœ… Confusion matrix saved to {save_path}")
            
            return fig
        else:
            fmt = 'd'
        
        sns.heatmap(cm, annot=True, fmt=fmt, cmap=cmap,
                   xticklabels=self.class_names,
                   yticklabels=self.class_names,
                   cbar_kws={'label': 'Normalized' if normalize else 'Count'},
                   ax=ax)
        
        ax.set_xlabel('Predicted Label', fontsize=12, fontweight='bold')
        ax.set_ylabel('True Label', fontsize=12, fontweight='bold')
        title = 'Normalized Confusion Matrix' if normalize else 'Confusion Matrix'
        ax.set_title(title, fontsize=14, fontweight='bold', pad=20)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"âœ… Confusion matrix saved to {save_path}")
        
        return fig
    
    def plot_metrics_comparison(self, 
                               figsize: Tuple[int, int] = (14, 6),
                               save_path: Optional[str] = None) -> plt.Figure:
        """
        í´ë˜ìŠ¤ë³„ ë©”íŠ¸ë¦­ ë¹„êµ ì‹œê°í™”
        
        Args:
            figsize: Figure í¬ê¸°
            save_path: ì €ì¥ ê²½ë¡œ
        
        Returns:
            Figure object
        """
        if self.metrics is None:
            self.calculate_metrics()
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)
        
        # Per-class Accuracy
        classes = list(self.metrics['per_class_accuracy'].keys())
        accuracies = list(self.metrics['per_class_accuracy'].values())
        
        bars1 = ax1.bar(range(len(classes)), accuracies, color='skyblue', edgecolor='navy', alpha=0.7)
        ax1.set_xticks(range(len(classes)))
        ax1.set_xticklabels(classes, rotation=45, ha='right')
        ax1.set_xlabel('Class', fontweight='bold', fontsize=11)
        ax1.set_ylabel('Accuracy', fontweight='bold', fontsize=11)
        ax1.set_title('Per-Class Accuracy', fontweight='bold', pad=10, fontsize=13)
        ax1.set_ylim([0, 1.05])
        ax1.grid(axis='y', alpha=0.3, linestyle='--')
        ax1.axhline(y=self.metrics['overall_accuracy'], color='red', linestyle='--', 
                   label=f"Overall: {self.metrics['overall_accuracy']:.3f}", linewidth=2)
        ax1.legend()
        
        # ê°’ í‘œì‹œ
        for bar, acc in zip(bars1, accuracies):
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height,
                    f'{acc:.3f}', ha='center', va='bottom', fontsize=9)
        
        # Per-class F1 Score
        f1_scores = list(self.metrics['per_class_f1'].values())
        
        bars2 = ax2.bar(range(len(classes)), f1_scores, color='lightcoral', edgecolor='darkred', alpha=0.7)
        ax2.set_xticks(range(len(classes)))
        ax2.set_xticklabels(classes, rotation=45, ha='right')
        ax2.set_xlabel('Class', fontweight='bold', fontsize=11)
        ax2.set_ylabel('F1 Score', fontweight='bold', fontsize=11)
        ax2.set_title('Per-Class F1 Score', fontweight='bold', pad=10, fontsize=13)
        ax2.set_ylim([0, 1.05])
        ax2.grid(axis='y', alpha=0.3, linestyle='--')
        ax2.axhline(y=self.metrics['f1_macro'], color='red', linestyle='--', 
                   label=f"Macro: {self.metrics['f1_macro']:.3f}", linewidth=2)
        ax2.legend()
        
        # ê°’ í‘œì‹œ
        for bar, f1 in zip(bars2, f1_scores):
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height,
                    f'{f1:.3f}', ha='center', va='bottom', fontsize=9)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"âœ… Metrics comparison saved to {save_path}")
        
        return fig
    
    def plot_roc_curves(self, 
                       figsize: Tuple[int, int] = (10, 8),
                       save_path: Optional[str] = None) -> plt.Figure:
        """
        ROC Curve ì‹œê°í™” (multi-class)
        
        Args:
            figsize: Figure í¬ê¸°
            save_path: ì €ì¥ ê²½ë¡œ
        
        Returns:
            Figure object
        """
        if self.y_proba is None:
            print("âš ï¸ Probability scores not available. Skipping ROC curve.")
            return None
        
        from sklearn.metrics import roc_curve, auc
        from sklearn.preprocessing import label_binarize
        
        # One-hot encoding
        y_true_bin = label_binarize(self.y_true, classes=range(len(self.class_names)))
        
        fig, ax = plt.subplots(figsize=figsize)
        
        # ê° í´ë˜ìŠ¤ë³„ ROC ê³¡ì„ 
        for i, class_name in enumerate(self.class_names):
            fpr, tpr, _ = roc_curve(y_true_bin[:, i], self.y_proba[:, i])
            roc_auc = auc(fpr, tpr)
            
            ax.plot(fpr, tpr, lw=2, 
                   label=f'{class_name} (AUC = {roc_auc:.3f})')
        
        # ëŒ€ê°ì„  (random classifier)
        ax.plot([0, 1], [0, 1], 'k--', lw=2, label='Random (AUC = 0.500)')
        
        ax.set_xlim([0.0, 1.0])
        ax.set_ylim([0.0, 1.05])
        ax.set_xlabel('False Positive Rate', fontweight='bold', fontsize=12)
        ax.set_ylabel('True Positive Rate', fontweight='bold', fontsize=12)
        ax.set_title('ROC Curves - Multi-Class', fontweight='bold', fontsize=14, pad=15)
        ax.legend(loc="lower right", fontsize=10)
        ax.grid(alpha=0.3)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"âœ… ROC curves saved to {save_path}")
        
        return fig
    
    def generate_report(self, output_path: Optional[str] = None) -> pd.DataFrame:
        """
        ë©”íŠ¸ë¦­ì„ DataFrameìœ¼ë¡œ ì •ë¦¬
        
        Args:
            output_path: CSV ì €ì¥ ê²½ë¡œ
        
        Returns:
            DataFrame containing metrics
        """
        if self.metrics is None:
            self.calculate_metrics()
        
        report_data = []
        
        # Per-class metrics
        for idx, class_name in enumerate(self.class_names):
            support = np.sum(self.y_true == idx)
            report_data.append({
                'Class': class_name,
                'Accuracy': self.metrics['per_class_accuracy'].get(class_name, 0),
                'F1-Score': self.metrics['per_class_f1'].get(class_name, 0),
                'Support': support
            })
        
        df = pd.DataFrame(report_data)
        
        # Overall metrics
        overall_row = pd.DataFrame([{
            'Class': 'Overall',
            'Accuracy': self.metrics['overall_accuracy'],
            'F1-Score': self.metrics['f1_macro'],
            'Support': len(self.y_true)
        }])
        
        df = pd.concat([df, overall_row], ignore_index=True)
        
        if output_path:
            df.to_csv(output_path, index=False)
            print(f"âœ… Report saved to {output_path}")
        
        return df
    
    def save_results(self, save_dir: Union[str, Path]):
        """
        ëª¨ë“  ê²°ê³¼ë¥¼ ì €ì¥
        
        Args:
            save_dir: ì €ì¥ ë””ë ‰í† ë¦¬
        """
        save_dir = Path(save_dir)
        save_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"\nğŸ’¾ Saving results to {save_dir}...")
        
        # ë©”íŠ¸ë¦­ JSON ì €ì¥
        metrics_path = save_dir / 'metrics.json'
        with open(metrics_path, 'w') as f:
            # numpy arrayë¥¼ listë¡œ ë³€í™˜
            metrics_serializable = {
                'overall_accuracy': float(self.metrics['overall_accuracy']),
                'f1_macro': float(self.metrics['f1_macro']),
                'f1_weighted': float(self.metrics['f1_weighted']),
                'per_class_accuracy': {k: float(v) for k, v in self.metrics['per_class_accuracy'].items()},
                'per_class_f1': {k: float(v) for k, v in self.metrics['per_class_f1'].items()},
                'precision_recall': {k: float(v) for k, v in self.metrics['precision_recall'].items()},
                'confusion_matrix': self.metrics['confusion_matrix'].tolist(),
                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
            json.dump(metrics_serializable, f, indent=4)
        print(f"  âœ… Metrics saved to {metrics_path}")
        
        # Predictions ì €ì¥
        predictions_path = save_dir / 'predictions.npz'
        np.savez(predictions_path, 
                y_true=self.y_true, 
                y_pred=self.y_pred,
                y_proba=self.y_proba if self.y_proba is not None else np.array([]))
        print(f"  âœ… Predictions saved to {predictions_path}")
        
        # CSV report ì €ì¥
        report_path = save_dir / 'evaluation_report.csv'
        self.generate_report(output_path=str(report_path))
        
        # Classification report ì €ì¥
        clf_report_path = save_dir / 'classification_report.txt'
        with open(clf_report_path, 'w') as f:
            report = get_classification_report(
                self.y_true, 
                self.y_pred, 
                self.class_names, 
                output_dict=False
            )
            f.write(report)
        print(f"  âœ… Classification report saved to {clf_report_path}")
    
    def full_evaluation(self, 
                       save_dir: Optional[Union[str, Path]] = None,
                       show_plots: bool = True,
                       save_plots: bool = True) -> Dict:
        """
        ì „ì²´ í‰ê°€ ìˆ˜í–‰ (ì˜ˆì¸¡ + ë©”íŠ¸ë¦­ ê³„ì‚° + ì‹œê°í™” + ì €ì¥)
        
        Args:
            save_dir: ì €ì¥ ë””ë ‰í† ë¦¬
            show_plots: í”Œë¡¯ í‘œì‹œ ì—¬ë¶€
            save_plots: í”Œë¡¯ ì €ì¥ ì—¬ë¶€
        
        Returns:
            Dictionary containing all metrics
        """
        # 1. ì˜ˆì¸¡ (í•„ìš”í•œ ê²½ìš°)
        if self.y_true is None and self.model is not None:
            self.predict_from_loader()
        
        # 2. ë©”íŠ¸ë¦­ ê³„ì‚°
        self.calculate_metrics()
        
        # 3. ê²°ê³¼ ì¶œë ¥
        self.print_summary()
        self.print_classification_report()
        
        # 4. ì‹œê°í™”
        if save_dir:
            save_dir = Path(save_dir)
            save_dir.mkdir(parents=True, exist_ok=True)
        
        # Confusion Matrix
        cm_path = save_dir / 'confusion_matrix.png' if save_plots and save_dir else None
        self.plot_confusion_matrix(save_path=cm_path, show_percentages=True)
        
        # Metrics Comparison
        metrics_path = save_dir / 'metrics_comparison.png' if save_plots and save_dir else None
        self.plot_metrics_comparison(save_path=metrics_path)
        
        # ROC Curves
        if self.y_proba is not None:
            roc_path = save_dir / 'roc_curves.png' if save_plots and save_dir else None
            self.plot_roc_curves(save_path=roc_path)
        
        # 5. ê²°ê³¼ ì €ì¥
        if save_dir:
            self.save_results(save_dir)
        
        # 6. DataFrame ì¶œë ¥
        df = self.generate_report()
        print("\n" + "="*70)
        print("ğŸ“Š METRICS TABLE")
        print("="*70)
        print(df.to_string(index=False))
        
        # 7. í”Œë¡¯ í‘œì‹œ
        if show_plots:
            plt.show()
        else:
            plt.close('all')
        
        return self.metrics


def evaluate_model(y_true: Optional[np.ndarray] = None,
                  y_pred: Optional[np.ndarray] = None,
                  class_names: Optional[List[str]] = None,
                  model: Optional[nn.Module] = None,
                  test_loader: Optional[DataLoader] = None,
                  device: str = 'cuda',
                  save_dir: Optional[str] = None,
                  show_plots: bool = True) -> ModelEvaluator:
    """
    ê°„í¸í•œ ëª¨ë¸ í‰ê°€ í•¨ìˆ˜
    
    Args:
        y_true: ì‹¤ì œ ë ˆì´ë¸” (ì˜µì…˜)
        y_pred: ì˜ˆì¸¡ ë ˆì´ë¸” (ì˜µì…˜)
        class_names: í´ë˜ìŠ¤ ì´ë¦„
        model: PyTorch ëª¨ë¸ (ì˜µì…˜)
        test_loader: í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë” (ì˜µì…˜)
        device: 'cuda' ë˜ëŠ” 'cpu'
        save_dir: ì €ì¥ ë””ë ‰í† ë¦¬
        show_plots: í”Œë¡¯ í‘œì‹œ ì—¬ë¶€
    
    Returns:
        ModelEvaluator instance
    """
    evaluator = ModelEvaluator(
        y_true=y_true,
        y_pred=y_pred,
        class_names=class_names,
        model=model,
        test_loader=test_loader,
        device=device
    )
    
    # ì „ì²´ í‰ê°€ ìˆ˜í–‰
    evaluator.full_evaluation(
        save_dir=save_dir,
        show_plots=show_plots,
        save_plots=True
    )
    
    return evaluator


def load_model_and_evaluate(model_path: str,
                           model_class: nn.Module,
                           test_loader: DataLoader,
                           class_names: List[str],
                           device: str = 'cuda',
                           save_dir: Optional[str] = None) -> ModelEvaluator:
    """
    ì €ì¥ëœ ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ì„œ í‰ê°€
    
    Args:
        model_path: ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
        model_class: ëª¨ë¸ í´ë˜ìŠ¤
        test_loader: í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”
        class_names: í´ë˜ìŠ¤ ì´ë¦„
        device: 'cuda' ë˜ëŠ” 'cpu'
        save_dir: ì €ì¥ ë””ë ‰í† ë¦¬
    
    Returns:
        ModelEvaluator instance
    """
    print(f"\nğŸ“¥ Loading model from {model_path}...")
    
    # ëª¨ë¸ ë¡œë“œ
    checkpoint = torch.load(model_path, map_location=device)
    model = model_class
    
    if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'])
        print(f"âœ… Model loaded (Epoch: {checkpoint.get('epoch', 'N/A')})")
    else:
        model.load_state_dict(checkpoint)
        print(f"âœ… Model loaded")
    
    # í‰ê°€
    return evaluate_model(
        model=model,
        test_loader=test_loader,
        class_names=class_names,
        device=device,
        save_dir=save_dir
    )


# ì‚¬ìš© ì˜ˆì œ
if __name__ == "__main__":
    # ì˜ˆì œ 1: ì´ë¯¸ ê³„ì‚°ëœ ì˜ˆì¸¡ê°’ìœ¼ë¡œ í‰ê°€
    print("="*70)
    print("EXAMPLE 1: Evaluation with predicted labels")
    print("="*70)
    
    np.random.seed(42)
    y_true = np.random.randint(0, 3, 200)
    y_pred = np.random.randint(0, 3, 200)
    class_names = ['Northern', 'Midlands', 'Southern']
    
    evaluator = ModelEvaluator(y_true, y_pred, class_names)
    evaluator.full_evaluation(
        save_dir='./results/example1',
        show_plots=False,
        save_plots=True
    )
    
    # ì˜ˆì œ 2: ê°„í¸ í•¨ìˆ˜ ì‚¬ìš©
    print("\n" + "="*70)
    print("EXAMPLE 2: Using convenience function")
    print("="*70)
    
    evaluate_model(
        y_true=y_true, 
        y_pred=y_pred, 
        class_names=class_names,
        save_dir='./results/example2',
        show_plots=False
    )
    
    print("\nâœ… All examples completed!")