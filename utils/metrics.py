# ============================================================================
# ğŸ‘¤ PERSON C: í‰ê°€ ë° ì‹¤í—˜ ê´€ë¦¬
# íŒŒì¼ 1: metrics.py
# ============================================================================

"""
í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚°
- Accuracy (overall, per-class)
- F1-score (macro, weighted)
- Confusion matrix
"""

import numpy as np
from sklearn.metrics import (
    accuracy_score,
    f1_score,
    confusion_matrix,
    classification_report
)
import torch

def compute_metrics(predictions, labels, label_type='region'):
    """
    ë©”íŠ¸ë¦­ ê³„ì‚°
    
    Args:
        predictions: (batch_size,) ë˜ëŠ” (batch_size, num_classes) 
                    - í›„ìë©´ argmax ì·¨í•¨
        labels: (batch_size,) ì •ìˆ˜ ë ˆì´ë¸”
        label_type: 'region' ë˜ëŠ” 'gender'
    
    Returns:
        dict: {
            'accuracy': float,
            'f1_macro': float,
            'f1_weighted': float,
            'confusion_matrix': np.array,
            'classification_report': str
        }
    """
    # TODO: êµ¬í˜„
    # 1. predictionsì´ logitsì´ë©´ argmax ì·¨í•˜ê¸°
    # 2. numpyë¡œ ë³€í™˜
    # 3. accuracy_score ê³„ì‚°
    # 4. f1_score ê³„ì‚° (macro, weighted)
    # 5. confusion_matrix ê³„ì‚°
    # 6. classification_report ìƒì„±
    # 7. dict í˜•íƒœ ë°˜í™˜
    pass

def compute_metrics_per_class(predictions, labels, class_names=None):
    """
    í´ë˜ìŠ¤ë³„ ë©”íŠ¸ë¦­
    
    Args:
        predictions: (batch_size,)
        labels: (batch_size,)
        class_names: í´ë˜ìŠ¤ ì´ë¦„ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ['irish', 'midlands', ...])
    
    Returns:
        dict: {
            'class_name': {
                'accuracy': float,
                'f1': float,
                'precision': float,
                'recall': float
            },
            ...
        }
    """
    # TODO: êµ¬í˜„
    pass